{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 01 Webscraping Clean\n",
    "\n",
    "#### Rens og filtrér hentet data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import bs4\n",
    "import csv\n",
    "import requests\n",
    "import re\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from tqdm.notebook import tqdm\n",
    "import random\n",
    "import pickle\n",
    "import time\n",
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "\n",
    "reviewErrorCount = [] # used in multithreadwritecsvfile\n",
    "dataDir = './data/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dumpFileNames():\n",
    "    pathname = dataDir + '/*.dump'\n",
    "    FileNames = []\n",
    "    for file in glob.glob(pathname, recursive=True):\n",
    "        file = file.replace(dataDir,'')\n",
    "        file = file.replace('.dump','')\n",
    "        FileNames.append(file)\n",
    "    \n",
    "    return FileNames\n",
    "\n",
    "def loadRawData(*args):\n",
    "    allPagesRequest = []\n",
    "    for arg in args:\n",
    "        with open(dataDir+arg+'.dump','rb') as file:\n",
    "            allPagesRequest = allPagesRequest + pickle.load(file)\n",
    "    return allPagesRequest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Frasortere emojis\n",
    "def remove_emojis(data):\n",
    "    emoj = re.compile(\"[\"\n",
    "        u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "        u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "        u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "        u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "        u\"\\U00002500-\\U00002BEF\"  # chinese char\n",
    "        u\"\\U00002702-\\U000027B0\"\n",
    "        u\"\\U00002702-\\U000027B0\"\n",
    "        u\"\\U000024C2-\\U0001F251\"\n",
    "        u\"\\U0001f926-\\U0001f937\"\n",
    "        u\"\\U00010000-\\U0010ffff\"\n",
    "        u\"\\u2640-\\u2642\" \n",
    "        u\"\\u2600-\\u2B55\"\n",
    "        u\"\\u200d\"\n",
    "        u\"\\u23cf\"\n",
    "        u\"\\u23e9\"\n",
    "        u\"\\u231a\"\n",
    "        u\"\\ufe0f\"  # dingbats\n",
    "        u\"\\u3030\"\n",
    "                      \"]+\", re.UNICODE)\n",
    "    return re.sub(emoj, '', data)\n",
    "\n",
    "# Frasortere linjeskift, comma, og dobbelt mellemrum & gør al tekst lowercase.\n",
    "def data_cleaning(data):\n",
    "    data = data.lower()\n",
    "    data = data.replace('\\r', ' ')\n",
    "    data = data.replace(\",\", ' ')\n",
    "    data = data.replace('\"', '')\n",
    "    data = data.replace('  ', '')\n",
    "    return data\n",
    "\n",
    "# Splitter hver url i listen, og returnere hjemmesidenavnet.\n",
    "def createFiles(url):\n",
    "    filename = str(url.url)\n",
    "    filename = filename.split('.')\n",
    "\n",
    "    if len(filename) == 4:\n",
    "        filename = filename[2].split('/')\n",
    "        filename = filename[2]\n",
    "    else:\n",
    "        filename = filename[3]\n",
    "    return filename\n",
    "\n",
    "# Tjekker om filen eksistere, så den ikke bliver appended til igen. \n",
    "def checkFileExists(filename):\n",
    "    try:\n",
    "        with open(filename,'rb') as file:\n",
    "            return 0\n",
    "    except:\n",
    "        return 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Filtrere den gå data med bs4 og skriver til en csv med det navn som request url'et indholder.\n",
    "def multithreadwritecsvfile(page):\n",
    "    loadingbar.update(1)\n",
    "\n",
    "    filename = dataDir+createFiles(page)+'.csv'\n",
    "    \n",
    "    with open(filename, 'a', newline='', encoding='utf-8') as output_file:\n",
    "        output_writer = csv.writer(output_file)\n",
    "        \n",
    "        soup = bs4.BeautifulSoup(page.content,'html.parser')\n",
    "        for content in soup.find_all('section', attrs={'class':'styles_reviewContentwrapper__zH_9M'}):\n",
    "            rating=None\n",
    "            splitted=None\n",
    "            review=None\n",
    "\n",
    "            try: \n",
    "                rating = content.find('img',alt = True)\n",
    "                splitted = rating.get('alt').split()\n",
    "            except Exception as e:\n",
    "                print('No rating found. Skipping...', e)\n",
    "\n",
    "            try: \n",
    "                review = content.find('p', attrs={'class':'typography_typography__QgicV typography_body__9UBeQ typography_color-black__5LYEn typography_weight-regular__TWEnf typography_fontstyle-normal__kHyN3'}).text\n",
    "                if len(review) > 2:\n",
    "                    review = remove_emojis(review)\n",
    "                    review = data_cleaning(review)\n",
    "                    if len(review) > 0:\n",
    "                        output_writer.writerow(['__label__'+splitted[2]+\" \", review])\n",
    "            except Exception as e:\n",
    "                reviewErrorCount.append(e);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# method to get the list of csv files we have avaliable\n",
    "def csvFileNames():\n",
    "    pathname = dataDir + '/*.csv'\n",
    "    FileNames = []\n",
    "    for file in glob.glob(pathname, recursive=True):\n",
    "        FileNames.append(file.replace(dataDir,''))\n",
    "    \n",
    "    return FileNames\n",
    "\n",
    "def mergeCsvFiles(fileNamesInput):\n",
    "\n",
    "    fileNames = []\n",
    "    # loop through the list with filenames we want to merge\n",
    "    for name in fileNamesInput:\n",
    "        fileNames.append(dataDir+name + '.csv')\n",
    "\n",
    "    # path for the joining files\n",
    "#     olddir = os.getcwd()\n",
    "#     os.chdir(dataDir)\n",
    "\n",
    "    #combine all the files\n",
    "    combinedCSV = pd.concat([pd.read_csv(f) for f in fileNames])\n",
    "    print(fileNames)\n",
    "    \n",
    "    #export to csv\n",
    "    combinedCSV.to_csv(dataDir+'combined_csv.csv', index=False, encoding='utf-8')\n",
    "#     os.chdir(olddir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Execute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dumpFileNames()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "allPagesRequest = loadRawData(\n",
    " 'boksen',\n",
    " 'contourdesign',\n",
    " 'cphbusiness',\n",
    " 'eboligskoedet',\n",
    " 'gamecastle',\n",
    " 'isports',\n",
    " 'jyskmobelfabrik',\n",
    " 'komplett',\n",
    " 'sas',\n",
    " 'tapeconnection',\n",
    " 'thyrep'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for shop in allPagesRequest:\n",
    "    filename = dataDir+createFiles(shop)+'.csv'\n",
    "    with open(filename, 'w', newline='', encoding='utf-8') as output_file:\n",
    "        output_writer = csv.writer(output_file)\n",
    "        output_writer.writerow(['rating','review'])\n",
    "        \n",
    "with tqdm(total=len(allPagesRequest)) as loadingbar: \n",
    "    with ThreadPoolExecutor(8) as ex:\n",
    "        \n",
    "        ex.map(multithreadwritecsvfile, allPagesRequest)\n",
    "\n",
    "print('Done. Skipped reviews:', len(reviewErrorCount))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "csvFileNames()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mergeCsvFiles([\n",
    " 'boksen',\n",
    " 'contourdesign',\n",
    " 'cphbusiness',\n",
    " 'eboligskoedet',\n",
    " 'gamecastle',\n",
    " 'isports',\n",
    " 'jyskmobelfabrik',\n",
    " 'komplett',\n",
    " 'sas',\n",
    " 'tapeconnection',\n",
    " 'thyrep'\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "d4d1e4263499bec80672ea0156c357c1ee493ec2b1c70f0acce89fc37c4a6abe"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

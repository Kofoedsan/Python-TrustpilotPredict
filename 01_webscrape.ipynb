{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I dette projekt vil vi gøre det muligt, at forudsige en karakter fra 1-5 ud fra tekstindholdet i en anmeldelse på Trustpilot,\n",
    "og vise om sammenhængen af én virksomheds samlede vurdering passer mellem karakter og anmeldelser.\n",
    "Vi har tænkt os at gøre dette ved at scrape en masse andmeldelser fra forskellige virksomheder på Truspilot ved brug af selenium. Vi vil basere vores model ud fra de indsamlede data.\n",
    "\n",
    "Med vores model vil vi:\n",
    "\n",
    "1. Evaluére og fastslå hvor brugbar modellen er ved at sammenligne træningsdata- og valideringsdata.\n",
    "<br>1a. Efterjustér modellens definition efter behov.\n",
    "\n",
    "\n",
    "2. Forudsige hvilken karakter en anmelder vil give ud fra den anmeldselse de har skrevet. (Regression)\n",
    "<br>2a. Sammenlign med den aktuelle anmelder karakter.\n",
    "\n",
    "\n",
    "3. Vis sansynligheden for predictions indenfor Trustpilots fem katagorier: Fremragende, God, Middel, Uder middel, Dårlig, for en specifik anmeldelse. (Classification)\n",
    "\n",
    "\n",
    "4. Gruppere og plot ratings for én specifik virksomhed i et 3D feature space. (Clustering)\n",
    "\n",
    "\n",
    "5. Brug WordCloud til og plotte de mest anvendte Negative og Positive ord."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%pip install requests==2.27.1\n",
    "\n",
    "import bs4\n",
    "import csv\n",
    "import requests\n",
    "import re\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from tqdm.notebook import tqdm\n",
    "import random\n",
    "import pickle\n",
    "\n",
    "workingProxyList=[]\n",
    "allShopsRequest = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OK\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "allShopsRequest.append(\"https://dk.trustpilot.com/review/www.proshop.dk\")\n",
    "allShopsRequest.append(\"https://dk.trustpilot.com/review/www.elgiganten.dk\")\n",
    "allShopsRequest.append(\"https://dk.trustpilot.com/review/power.dk\")\n",
    "allShopsRequest.append(\"https://dk.trustpilot.com/review/www.yousee.dk\")\n",
    "allShopsRequest.append(\"https://dk.trustpilot.com/review/www.telenor.dk\")\n",
    "\n",
    "print(\"OK\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('ipList', 'rb') as file:\n",
    "    workingProxyList = pickle.load(file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "totalpageprShop=[]\n",
    "\n",
    "def multiThreadGetTotalPages(shop):\n",
    "    loadingbar.update(1)\n",
    "    listofpages=\"\"\n",
    "    totalpages=None\n",
    "    req = requests.get(shop, timeout=10)\n",
    "    soup = bs4.BeautifulSoup(req.content, 'html.parser')\n",
    "    \n",
    "    try:      \n",
    "        totalpages = soup.find('a', attrs={'name':'pagination-button-last'}).text\n",
    "        shopAndPage = {'shop': shop,'pages': totalpages}\n",
    "        totalpageprShop.append(shopAndPage)\n",
    "    except:       \n",
    "        listofpages = [7,6,5,4,3,2,1]\n",
    "        try:\n",
    "            for content in listofpages:\n",
    "                totalpages = soup.find('a', attrs={'name':'pagination-button-'+str(content)}).text\n",
    "                if totalpages is not None:\n",
    "                    print('Amount of review pages found:', totalpages)\n",
    "                    shopAndPage = {'shop': shop,'pages': totalpages}\n",
    "                    totalpageprShop.append(shopAndPage)\n",
    "                    break\n",
    "                \n",
    "        except Exception as e:\n",
    "            print('Error', e)\n",
    " \n",
    "        \n",
    "with tqdm(total=len(allShopsRequest)) as loadingbar: \n",
    "    with ThreadPoolExecutor(2) as ex:\n",
    "        ex.map(multiThreadGetTotalPages, allShopsRequest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "allPagesRequest = []\n",
    "\n",
    "def multiThreadDownloader(shop,page):\n",
    "    loadingbar.update(1)\n",
    "    \n",
    "    url = shop+'?page='+str(page)\n",
    "    \n",
    "    #Overvej at droppe proxy.. Servers er langsomme, timer ofte ud eller virker slet ikke. \n",
    "    #Selvom de er testet 5min forinden.\n",
    "    if len(workingProxyList) > 0:\n",
    "        for ip in workingProxyList:\n",
    "            try:\n",
    "                randomIpNumber = random.randint(0,len(workingProxyList))\n",
    "                allPagesRequest.append(requests.get(url, timeout=15, proxies=workingProxyList[randomIpNumber]))\n",
    "            except:\n",
    "                pass\n",
    "    else:\n",
    "        allPagesRequest.append(requests.get(url, timeout=5))\n",
    "    time.sleep(10)\n",
    "\n",
    "    \n",
    "    \n",
    "\n",
    "allpages=0    \n",
    "for x in totalpageprShop:\n",
    "    allpages = allpages+int(x['pages'])\n",
    "\n",
    "\n",
    "with tqdm(total=allpages) as loadingbar:\n",
    "    print(len(workingProxyList))\n",
    "    for i in totalpageprShop:\n",
    "        digitlist = []\n",
    "        res = int(i['pages'])\n",
    "        for digit in range(res):\n",
    "            digitlist.append(digit)\n",
    "        with ThreadPoolExecutor(2) as ex:\n",
    "            ex.map(multiThreadDownloader,[i['shop']] * len(digitlist), digitlist)\n",
    "        \n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_emojis(data):\n",
    "    emoj = re.compile(\"[\"\n",
    "        u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "        u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "        u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "        u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "        u\"\\U00002500-\\U00002BEF\"  # chinese char\n",
    "        u\"\\U00002702-\\U000027B0\"\n",
    "        u\"\\U00002702-\\U000027B0\"\n",
    "        u\"\\U000024C2-\\U0001F251\"\n",
    "        u\"\\U0001f926-\\U0001f937\"\n",
    "        u\"\\U00010000-\\U0010ffff\"\n",
    "        u\"\\u2640-\\u2642\" \n",
    "        u\"\\u2600-\\u2B55\"\n",
    "        u\"\\u200d\"\n",
    "        u\"\\u23cf\"\n",
    "        u\"\\u23e9\"\n",
    "        u\"\\u231a\"\n",
    "        u\"\\ufe0f\"  # dingbats\n",
    "        u\"\\u3030\"\n",
    "                      \"]+\", re.UNICODE)\n",
    "    return re.sub(emoj, '', data)\n",
    "\n",
    "print(\"OK\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "reviewErrorCount = [];\n",
    "\n",
    "def multithreadwritecsvfile(page):\n",
    "    loadingbar.update(1)\n",
    "    with open('test.csv', 'a', newline='', encoding='utf-8') as output_file:\n",
    "        output_writer = csv.writer(output_file)\n",
    "        soup = bs4.BeautifulSoup(page.content,'html.parser')\n",
    "        for content in soup.find_all('section', attrs={'class':'styles_reviewContentwrapper__zH_9M'}):\n",
    "            rating=None\n",
    "            splitted=None\n",
    "            title=None\n",
    "            review=None\n",
    "            \n",
    "            try: \n",
    "                rating = content.find('img',alt = True)\n",
    "                splitted = rating.get('alt').split()\n",
    "            except Exception as e:\n",
    "                print('No rating found. Skipping...', e)\n",
    "            \n",
    "            try: \n",
    "                title = content.find('a', attrs={'name':'review-title'}).text\n",
    "                title = remove_emojis(title)\n",
    "            except Exception as e:\n",
    "                print('No title found. Skipping...', e)\n",
    "                \n",
    "            try: \n",
    "                review = content.find('p', attrs={'class':'typography_typography__QgicV typography_body__9UBeQ typography_color-black__5LYEn typography_weight-regular__TWEnf typography_fontstyle-normal__kHyN3'}).text\n",
    "                if len(review) > 2:\n",
    "                    review = remove_emojis(review)\n",
    "                    review = review.replace(\",\", \" \")\n",
    "                    review = review.replace(' \"\" ', \"\")\n",
    "                    review = review.replace('  ', \"\")\n",
    "                    review = review.replace(' \\n ', \"\")\n",
    "                    output_writer.writerow(['__label__'+splitted[2]+\" \", review])\n",
    "            except Exception as e:\n",
    "                #print('No review found. Skipping...', e)\n",
    "                reviewErrorCount.append(e);\n",
    "            \n",
    "\n",
    "with tqdm(total=len(allPagesRequest)) as loadingbar: \n",
    "    with ThreadPoolExecutor(8) as ex:\n",
    "        with open('test.csv', 'w', newline='', encoding='utf-8') as output_file:\n",
    "            output_writer = csv.writer(output_file)\n",
    "            output_writer.writerow(['rating','review'])\n",
    "        ex.map(multithreadwritecsvfile, allPagesRequest)\n",
    "\n",
    "print('Done. Skipped reviews:', len(reviewErrorCount))"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "369f2c481f4da34e4445cda3fffd2e751bd1c4d706f27375911949ba6bb62e1c"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
